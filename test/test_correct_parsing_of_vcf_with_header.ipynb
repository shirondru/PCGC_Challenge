{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5276ef92",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3628e0470044>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0msummed_scores_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0mmaxed_scores_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0mreference_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0malternate_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-3628e0470044>\u001b[0m in \u001b[0;36mvariant_centered_sequences\u001b[0;34m(vcf_file, sequence_length, gzipped, chr_prefix)\u001b[0m\n\u001b[1;32m    183\u001b[0m     reference_sequence=FastaStringExtractor(fasta_file))\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mvariant\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariant_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvcf_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgzipped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgzipped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     interval = Interval(chr_prefix + variant.chrom,\n\u001b[1;32m    187\u001b[0m                         variant.pos, variant.pos)\n",
      "\u001b[0;32m<ipython-input-3-3628e0470044>\u001b[0m in \u001b[0;36mvariant_generator\u001b[0;34m(vcf_file, gzipped, skip_lines, max_lines)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mchrom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Split ALT alleles and return individual variants as output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from joblib import Parallel,delayed\n",
    "import tensorflow_hub as hub\n",
    "import joblib\n",
    "import gzip\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import biomart\n",
    "from scipy.stats import zscore\n",
    "from pandas import HDFStore\n",
    "import h5py\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Run Enformer on variants associated with diseases (via the GWAS catalog) relevant to the PsychENCODE project.', prog='EnformerPsychENCODE_GWAS_predictions.py')\n",
    "# parser.add_argument('--vcf_path', required=True, type=str,  help='Full path to the vcf file containing the mutations to be run by Enformer')\n",
    "# parser.add_argument('--disease', required=True, type=str,  help='Name of the disease with which these mutations are associated')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# vcf_path = str(args.vcf_path)\n",
    "# disease = str(args.disease)\n",
    "\n",
    "model_path = \"/wynton/home/hernandez/shirondru/pollard_lab/enformer\"\n",
    "fasta_file = '/wynton/home/hernandez/shirondru/pollard_lab/data/hg38_genome.fa'\n",
    "clinvar_vcf = '/wynton/home/hernandez/shirondru/pollard_lab/data/clinvar.vcf.gz'\n",
    "\n",
    "\n",
    "# cols = ['chrom','txStart','txEnd','ENST','strand',,'cdsStart','cdsEnd','exonCount','exonStarts','exonEnds','ENST_y','A','B','C','D','Gene','E','F']\n",
    "gene_annotations = pd.read_csv(\"/wynton/home/hernandez/shirondru/pollard_lab/data/knownGene.tsv\",sep = '\\t') #from ucsc genome browser hg38\n",
    "\n",
    "\n",
    "# Download targets from Basenji2 dataset \n",
    "# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).\n",
    "df_targets = pd.read_csv(\"/wynton/home/hernandez/shirondru/pollard_lab/data/enformer_df_targets.csv\")\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\n",
    "SEQUENCE_LENGTH = 393216\n",
    "\n",
    "class Enformer:\n",
    "\n",
    "  def __init__(self, tfhub_url):\n",
    "    self._model = hub.load(tfhub_url).model\n",
    "\n",
    "  def predict_on_batch(self, inputs):\n",
    "    predictions = self._model.predict_on_batch(inputs)\n",
    "    return {k: v.numpy() for k, v in predictions.items()}\n",
    "\n",
    "  @tf.function\n",
    "  def contribution_input_grad(self, input_sequence,\n",
    "                              target_mask, output_head='human'):\n",
    "    input_sequence = input_sequence[tf.newaxis]\n",
    "\n",
    "    target_mask_mass = tf.reduce_sum(target_mask)\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(input_sequence)\n",
    "      prediction = tf.reduce_sum(\n",
    "          target_mask[tf.newaxis] *\n",
    "          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n",
    "\n",
    "    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n",
    "    input_grad = tf.squeeze(input_grad, axis=0)\n",
    "    return tf.reduce_sum(input_grad, axis=-1)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsRaw:\n",
    "\n",
    "  def __init__(self, tfhub_url, organism='human'):\n",
    "    self._model = Enformer(tfhub_url)\n",
    "    self._organism = organism\n",
    "  \n",
    "  def predict_on_batch(self, inputs):\n",
    "    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n",
    "    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n",
    "\n",
    "    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsNormalized:\n",
    "\n",
    "  def __init__(self, tfhub_url, transform_pkl_path,\n",
    "               organism='human'):\n",
    "    assert organism == 'human', 'Transforms only compatible with organism=human'\n",
    "    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
    "    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n",
    "      transform_pipeline = joblib.load(f)\n",
    "    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n",
    "    \n",
    "  def predict_on_batch(self, inputs):\n",
    "    scores = self._model.predict_on_batch(inputs)\n",
    "    return self._transform.transform(scores)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsPCANormalized:\n",
    "\n",
    "  def __init__(self, tfhub_url, transform_pkl_path,\n",
    "               organism='human', num_top_features=500):\n",
    "    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
    "    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n",
    "      self._transform = joblib.load(f)\n",
    "    self._num_top_features = num_top_features\n",
    "    \n",
    "  def predict_on_batch(self, inputs):\n",
    "    scores = self._model.predict_on_batch(inputs)\n",
    "    return self._transform.transform(scores)[:, :self._num_top_features]\n",
    "\n",
    "\n",
    "# TODO(avsec): Add feature description: Either PCX, or full names.\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "# @title `variant_centered_sequences`\n",
    "\n",
    "# @title `variant_centered_sequences`\n",
    "\n",
    "class FastaStringExtractor:\n",
    "    \n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),\n",
    "                                    )\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "\n",
    "\n",
    "def variant_generator(vcf_file, gzipped=False,skip_lines = 0,max_lines = np.inf):\n",
    "  \"\"\"Yields a kipoiseq.dataclasses.Variant for each row in VCF file.\"\"\"\n",
    "  def _open(file):\n",
    "    return gzip.open(vcf_file, 'rt') if gzipped else open(vcf_file)\n",
    "    \n",
    "  with _open(vcf_file) as f:\n",
    "    for idx,line in enumerate(f):\n",
    "      if line.startswith('#') or line.startswith('CHROM'):\n",
    "        continue\n",
    "      \n",
    "      chrom, pos, id, ref, alt_list = line.split('\\t')[:5]\n",
    "\n",
    "        # Split ALT alleles and return individual variants as output.\n",
    "      for alt in alt_list.split(','):\n",
    "        yield kipoiseq.dataclasses.Variant(chrom=chrom, pos=pos,\n",
    "                                           ref=ref, alt=alt, id=id)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
    "\n",
    "\n",
    "def variant_centered_sequences(vcf_file, sequence_length, gzipped=False,\n",
    "                               chr_prefix=''):\n",
    "  seq_extractor = kipoiseq.extractors.VariantSeqExtractor(\n",
    "    reference_sequence=FastaStringExtractor(fasta_file))\n",
    "\n",
    "  for variant in variant_generator(vcf_file, gzipped=gzipped):\n",
    "    interval = Interval(chr_prefix + variant.chrom,\n",
    "                        variant.pos, variant.pos)\n",
    "    interval = interval.resize(sequence_length)\n",
    "    center = interval.center() - interval.start\n",
    "\n",
    "    reference = seq_extractor.extract(interval, [], anchor=center)\n",
    "    alternate = seq_extractor.extract(interval, [variant], anchor=center)\n",
    "\n",
    "    yield {'inputs': {'ref': one_hot_encode(reference),\n",
    "                      'alt': one_hot_encode(alternate)},\n",
    "           'metadata': {'chrom': chr_prefix + variant.chrom,\n",
    "                        'pos': variant.pos,\n",
    "                        'id': variant.id,\n",
    "                        'ref': variant.ref,\n",
    "                        'alt': variant.alt}}\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "model = Enformer(model_path)\n",
    "fasta_extractor = FastaStringExtractor(fasta_file)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "\n",
    "#take variable number of MAF>0.05 1KG variants and get model(alt) - model(ref) predictions\n",
    "#take sum or max along sequence axis to get variant score for each track. Save these scores + variant position and allele metadata\n",
    "\n",
    "\n",
    "vcf_path=\"/wynton/home/hernandez/shirondru/pollard_lab/GWASPredictions/getGWASVariants/GWAS_psychENCODE_LeadTagVariants/ADHD/AkitaEnformer/PsychENCODE_GWASVariants_ADHD.vcf00.vcf\"\n",
    "vcf_basename = vcf_path.split('/')[-1]\n",
    "\n",
    "it = variant_centered_sequences(vcf_path, sequence_length=SEQUENCE_LENGTH,\n",
    "                            gzipped=False, chr_prefix='')\n",
    "summed_scores_list = []\n",
    "maxed_scores_list = []\n",
    "for idx, example in enumerate(it):\n",
    "    reference_prediction = model.predict_on_batch({k: v[tf.newaxis] for k,v in example['inputs'].items()}['ref'])['human'][0]\n",
    "    alternate_prediction = model.predict_on_batch({k: v[tf.newaxis] for k,v in example['inputs'].items()}['alt'])['human'][0]\n",
    "    pred = alternate_prediction - reference_prediction\n",
    "    summed_scores = np.sum(pred,axis = 0)\n",
    "    maxed_scores = np.max(abs(pred),axis = 0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40dfb89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping header line: ##fileformat=VCFv4.1\n",
      "\n",
      "skipping header line:     ##fileDate=20220718\n",
      "\n",
      "skipping header line:     ##source=/wynton/home/hernandez/shirondru/pollard_lab/GWASPredictions/getGWASVariants/GetPsychENCODEVariants_TroubleshootOpenTargetsGenetics.ipynb\n",
      "\n",
      "skipping header line:     ##reference=/wynton/home/hernandez/shirondru/pollard_lab/data/hg38_genome.fa\n",
      "\n",
      "skipping header line:     #CHROM POS ID REF ALT QUAL FILTER INFO\n",
      "\n",
      "skipping header line:     CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gzipped=False\n",
    "def _open(file):\n",
    "    return gzip.open(vcf_path, 'rt') if gzipped else open(vcf_path)\n",
    "    \n",
    "with _open(vcf_path) as f:\n",
    "    for idx,line in enumerate(f):\n",
    "        if line.startswith('#') or line.startswith('CHROM') or line.startswith('    '):\n",
    "            print(f\"skipping header line: {line}\")\n",
    "    chrom, pos, id, ref, alt_list = line.split('\\t')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dea7a5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4524b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pollard_environment",
   "language": "python",
   "name": "pollard_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
