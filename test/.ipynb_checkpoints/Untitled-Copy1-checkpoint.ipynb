{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5b39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from joblib import Parallel,delayed\n",
    "import tensorflow_hub as hub\n",
    "import joblib\n",
    "import gzip\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import biomart\n",
    "from scipy.stats import zscore\n",
    "from pandas import HDFStore\n",
    "import h5py\n",
    "import itertools\n",
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "\n",
    "vcf_path =\"/wynton/home/hernandez/shirondru/pollard_lab/GWASPredictions/getGWASVariants/GWAS_psychENCODE_LeadTagVariants/Alzheimer/AkitaEnformer/PsychENCODE_GWASVariants_Alzheimer.vcf11.vcf\"\n",
    "disease = \"Alzheimer\"\n",
    "\n",
    "model_path = \"/wynton/home/hernandez/shirondru/pollard_lab/enformer\"\n",
    "fasta_file = '/wynton/home/hernandez/shirondru/pollard_lab/data/hg38_genome.fa'\n",
    "clinvar_vcf = '/wynton/home/hernandez/shirondru/pollard_lab/data/clinvar.vcf.gz'\n",
    "\n",
    "\n",
    "# cols = ['chrom','txStart','txEnd','ENST','strand',,'cdsStart','cdsEnd','exonCount','exonStarts','exonEnds','ENST_y','A','B','C','D','Gene','E','F']\n",
    "gene_annotations = pd.read_csv(\"/wynton/home/hernandez/shirondru/pollard_lab/data/knownGene.tsv\",sep = '\\t') #from ucsc genome browser hg38\n",
    "\n",
    "\n",
    "# Download targets from Basenji2 dataset \n",
    "# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).\n",
    "df_targets = pd.read_csv(\"/wynton/home/hernandez/shirondru/pollard_lab/data/enformer_df_targets.csv\")\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\n",
    "SEQUENCE_LENGTH = 393216\n",
    "\n",
    "class Enformer:\n",
    "\n",
    "  def __init__(self, tfhub_url):\n",
    "    self._model = hub.load(tfhub_url).model\n",
    "\n",
    "  def predict_on_batch(self, inputs):\n",
    "    predictions = self._model.predict_on_batch(inputs)\n",
    "    return {k: v.numpy() for k, v in predictions.items()}\n",
    "\n",
    "  @tf.function\n",
    "  def contribution_input_grad(self, input_sequence,\n",
    "                              target_mask, output_head='human'):\n",
    "    input_sequence = input_sequence[tf.newaxis]\n",
    "\n",
    "    target_mask_mass = tf.reduce_sum(target_mask)\n",
    "    with tf.GradientTape() as tape:\n",
    "      tape.watch(input_sequence)\n",
    "      prediction = tf.reduce_sum(\n",
    "          target_mask[tf.newaxis] *\n",
    "          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n",
    "\n",
    "    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n",
    "    input_grad = tf.squeeze(input_grad, axis=0)\n",
    "    return tf.reduce_sum(input_grad, axis=-1)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsRaw:\n",
    "\n",
    "  def __init__(self, tfhub_url, organism='human'):\n",
    "    self._model = Enformer(tfhub_url)\n",
    "    self._organism = organism\n",
    "  \n",
    "  def predict_on_batch(self, inputs):\n",
    "    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n",
    "    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n",
    "\n",
    "    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsNormalized:\n",
    "\n",
    "  def __init__(self, tfhub_url, transform_pkl_path,\n",
    "               organism='human'):\n",
    "    assert organism == 'human', 'Transforms only compatible with organism=human'\n",
    "    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
    "    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n",
    "      transform_pipeline = joblib.load(f)\n",
    "    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n",
    "    \n",
    "  def predict_on_batch(self, inputs):\n",
    "    scores = self._model.predict_on_batch(inputs)\n",
    "    return self._transform.transform(scores)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsPCANormalized:\n",
    "\n",
    "  def __init__(self, tfhub_url, transform_pkl_path,\n",
    "               organism='human', num_top_features=500):\n",
    "    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
    "    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n",
    "      self._transform = joblib.load(f)\n",
    "    self._num_top_features = num_top_features\n",
    "    \n",
    "  def predict_on_batch(self, inputs):\n",
    "    scores = self._model.predict_on_batch(inputs)\n",
    "    return self._transform.transform(scores)[:, :self._num_top_features]\n",
    "\n",
    "\n",
    "# TODO(avsec): Add feature description: Either PCX, or full names.\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "# @title `variant_centered_sequences`\n",
    "\n",
    "# @title `variant_centered_sequences`\n",
    "\n",
    "class FastaStringExtractor:\n",
    "    \n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),\n",
    "                                    )\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "\n",
    "\n",
    "def variant_generator(vcf_file, gzipped=False,skip_lines = 0,max_lines = np.inf):\n",
    "  \"\"\"Yields a kipoiseq.dataclasses.Variant for each row in VCF file.\"\"\"\n",
    "  def _open(file):\n",
    "    return gzip.open(vcf_file, 'rt') if gzipped else open(vcf_file)\n",
    "    \n",
    "  with _open(vcf_file) as f:\n",
    "    for idx,line in enumerate(f):\n",
    "      if line.startswith('#') or line.startswith('CHROM') or line.startswith('    '): # header lines start wth either '#', \"CHROM\" to denote the colnames row, or '    ' because of a formatting error when generating the headers for the vcf\n",
    "        continue\n",
    "      \n",
    "      chrom, pos, id, ref, alt_list = line.split('\\t')[:5]\n",
    "\n",
    "        # Split ALT alleles and return individual variants as output.\n",
    "      for alt in alt_list.split(','):\n",
    "        yield kipoiseq.dataclasses.Variant(chrom=chrom, pos=pos,\n",
    "                                           ref=ref, alt=alt, id=id)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
    "\n",
    "\n",
    "def variant_centered_sequences(vcf_file, sequence_length, gzipped=False,\n",
    "                               chr_prefix=''):\n",
    "  seq_extractor = kipoiseq.extractors.VariantSeqExtractor(\n",
    "    reference_sequence=FastaStringExtractor(fasta_file))\n",
    "\n",
    "  for variant in variant_generator(vcf_file, gzipped=gzipped):\n",
    "    interval = Interval(chr_prefix + variant.chrom,\n",
    "                        variant.pos, variant.pos)\n",
    "    interval = interval.resize(sequence_length)\n",
    "    center = interval.center() - interval.start\n",
    "\n",
    "    reference = seq_extractor.extract(interval, [], anchor=center)\n",
    "    alternate = seq_extractor.extract(interval, [variant], anchor=center)\n",
    "\n",
    "    yield {'inputs': {'ref': one_hot_encode(reference),\n",
    "                      'alt': one_hot_encode(alternate)},\n",
    "           'metadata': {'chrom': chr_prefix + variant.chrom,\n",
    "                        'pos': variant.pos,\n",
    "                        'id': variant.id,\n",
    "                        'ref': variant.ref,\n",
    "                        'alt': variant.alt}}\n",
    "# In[63]:\n",
    "\n",
    "\n",
    "model = Enformer(model_path)\n",
    "fasta_extractor = FastaStringExtractor(fasta_file)\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "\n",
    "#take variable number of MAF>0.05 1KG variants and get model(alt) - model(ref) predictions\n",
    "#take sum or max along sequence axis to get variant score for each track. Save these scores + variant position and allele metadata\n",
    "\n",
    "\n",
    "\n",
    "vcf_basename = \"PsychENCODE_GWASVariants_Alzheimer.vcf11.vcf\"\n",
    "\n",
    "it = variant_centered_sequences(vcf_path, sequence_length=SEQUENCE_LENGTH,\n",
    "                            gzipped=False, chr_prefix='')\n",
    "summed_scores_list = []\n",
    "maxed_scores_list = []\n",
    "for idx, example in enumerate(it):\n",
    "    if idx % 500 == 0:\n",
    "        print(idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('done')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########### DEPRACATED: FOR PARALLELIZATION #########\n",
    "\n",
    "\n",
    "\n",
    "# nslots = os.getenv('NSLOTS', '1')  # env var is always a 'str'\n",
    "# nslots = int(nslots)               # coerce to an 'int'\n",
    "# print(f'Number of slots available: {nslots}')\n",
    "\n",
    "\n",
    "# #PARALLELIZE ENFORMER PREDICTIONS ###\n",
    "# #parallelize first 5 chunks then second 5 chunks for each null_distribution size \n",
    "\n",
    "# n_variants = [100000,500000,1000000]\n",
    "# chunks1 = range(1,6)\n",
    "# chunks2 = range(6,11)\n",
    "\n",
    "# Parallel(n_jobs = nslots,backend='multiprocessing')(\n",
    "#     delayed(score_1KG_variants)(n,chunk) for n,chunk in itertools.product(n_variants, chunks1))\n",
    "# Parallel(n_jobs = nslots,backend='multiprocessing')(\n",
    "#     delayed(score_1KG_variants)(n,chunk) for n,chunk in itertools.product(n_variants, chunks2))\n",
    "# # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8da8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pollard_environment",
   "language": "python",
   "name": "pollard_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
